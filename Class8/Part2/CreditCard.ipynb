{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a239913a-b27f-15e7-8911-53b62bf41c4e"
   },
   "source": [
    "# Lab : Kaggle Credit card fraud detection\n",
    "\n",
    "It is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase.\n",
    "\n",
    "The dataset contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "\n",
    "**Objectives:** Compare Logistic Regression classifiers on skewed data. The idea is to compare if preprocessing techniques work better when there is an overwhelming majority class that can disrupt the efficiency of the predictive model. Learn how to apply cross validation (CV) for hyper-parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T16:05:43.790796Z",
     "start_time": "2018-11-03T16:05:43.778100Z"
    },
    "_cell_guid": "029ecde6-086d-7a8e-de44-363a7a23dbd8"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "warnings.filterwarnings('ignore',category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore',category=Warning)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b4de5f93-d467-ad7d-4597-03d5f3e89f86"
   },
   "source": [
    "### Load the anonimised dataset\n",
    "\n",
    "Dataset contains only numerical features which are the result of a PCA (Principal Component Analysis) transformation. Due to confidentiality issues, the original features and more personal information cannot be provided. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction quantity of money. \n",
    " \n",
    "The last column is the Class:  normal transaction (0),  fraud transaction (1). \n",
    "\n",
    "Load the dataset stored in the file *\"creditcard.csv\"*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T15:54:12.238585Z",
     "start_time": "2018-11-03T15:54:04.176102Z"
    },
    "_cell_guid": "7e5ca1e3-3597-19d2-b4be-dffd335df630"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time      9.481386e+04\n",
      "V1        1.759061e-12\n",
      "V2       -8.251130e-13\n",
      "V3       -9.654937e-13\n",
      "V4        8.321385e-13\n",
      "V5        1.649999e-13\n",
      "V6        4.248366e-13\n",
      "V7       -3.054600e-13\n",
      "V8        8.777971e-14\n",
      "V9       -1.179749e-12\n",
      "V10       7.092545e-13\n",
      "V11       1.874948e-12\n",
      "V12       1.053347e-12\n",
      "V13       7.127611e-13\n",
      "V14      -1.474791e-13\n",
      "V15      -5.231558e-13\n",
      "V16      -2.282250e-13\n",
      "V17      -6.425436e-13\n",
      "V18       4.950748e-13\n",
      "V19       7.057397e-13\n",
      "V20       1.766111e-12\n",
      "V21      -3.405756e-13\n",
      "V22      -5.723197e-13\n",
      "V23      -9.725856e-13\n",
      "V24       1.464150e-12\n",
      "V25      -6.987102e-13\n",
      "V26      -5.617874e-13\n",
      "V27       3.332082e-12\n",
      "V28      -3.518874e-12\n",
      "Amount    8.834962e+01\n",
      "Class     1.727486e-03\n",
      "dtype: float64\n",
      "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
      "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
      "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
      "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
      "\n",
      "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
      "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
      "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
      "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
      "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
      "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
      "\n",
      "        V26       V27       V28  Amount  Class  \n",
      "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
      "1  0.125895 -0.008983  0.014724    2.69      0  \n",
      "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
      "3 -0.221929  0.062723  0.061458  123.50      0  \n",
      "4  0.502292  0.219422  0.215153   69.99      0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"creditcard.csv\")\n",
    "\n",
    "\n",
    "# What is data dimension => (284807, 31)    \n",
    "means = data.mean()\n",
    "\n",
    "#Compute the mean of each column, and see that the anonimised features V1-V28 \n",
    "#have mean arround 0\n",
    "print(means)\n",
    "\n",
    "# show the first few rows from the dataset \n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize the values of Column Amount  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
      "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
      "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
      "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
      "\n",
      "         V8        V9       V10  ...       V21       V22       V23       V24  \\\n",
      "0  0.098698  0.363787  0.090794  ... -0.018307  0.277838 -0.110474  0.066928   \n",
      "1  0.085102 -0.255425 -0.166974  ... -0.225775 -0.638672  0.101288 -0.339846   \n",
      "2  0.247676 -1.514654  0.207643  ...  0.247998  0.771679  0.909412 -0.689281   \n",
      "3  0.377436 -1.387024 -0.054952  ... -0.108300  0.005274 -0.190321 -1.175575   \n",
      "4 -0.270533  0.817739  0.753074  ... -0.009431  0.798278 -0.137458  0.141267   \n",
      "\n",
      "        V25       V26       V27       V28  Class  normAmount  \n",
      "0  0.128539 -0.189115  0.133558 -0.021053      0    0.244964  \n",
      "1  0.167170  0.125895 -0.008983  0.014724      0   -0.342475  \n",
      "2 -0.327642 -0.139097 -0.055353 -0.059752      0    1.160686  \n",
      "3  0.647376 -0.221929  0.062723  0.061458      0    0.140534  \n",
      "4 -0.206010  0.502292  0.219422  0.215153      0   -0.073403  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "#x.reshape(-1, 1) does not mean normalizing between -1,1). \n",
    "#It means collumn vector (-1 means all rows), second dimension = 1.\n",
    "\n",
    "data['normAmount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))\n",
    "\n",
    "#drop column Time as irrelevant feature\n",
    "#drop columnt Amount as column normAmount was added\n",
    "data = data.drop(['Time','Amount'],axis=1)  \n",
    "\n",
    "# show again the first few rows of the normalized dataset \n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6268bbd8-6de5-2389-5693-ecd9a14872d4"
   },
   "source": [
    "#### Compute the number of samples per class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Class 1: 492\n",
      "Num Class 0: 284315\n"
     ]
    }
   ],
   "source": [
    "N_records_fraud = len(data[data.Class == 1])\n",
    "\n",
    "N_records_normal = len(data[data.Class == 0])\n",
    "\n",
    "# How many samples of Class 1 ( fraud transaction) ?   =>ANSWER: Class 1 = 492\n",
    "print(f\"Num Class 1: {N_records_fraud}\")\n",
    "\n",
    "# How many samples of Class 0 (normal transaction) ? => ANSWER: Class 0 = 284315\n",
    "print(f\"Num Class 0: {N_records_normal}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "801dd843-a90b-bb5f-97e7-2da84cc6cd6a"
   },
   "source": [
    "###  Data is totally unbalanced ! How to deal with such classification problem:\n",
    "\n",
    "- Collect more data.  Nice strategy but not always applicable. \n",
    "- Change the performance metric (do not rely only on Accuracy): compute other metrics Precision, Recall, F1_score.\n",
    "- Resampling the dataset to have an approximate 50-50 ratio:\n",
    "    - By OVER-sampling => add copies of the under-represented class.\n",
    "    - By UNDER-sampling => delete instances from the over-represented class.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cfffc4c5-b621-250f-3b6b-6118cef52b9d"
   },
   "source": [
    "Extract the features in matrix X and the class labels in vector y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T15:54:06.954000Z",
     "start_time": "2018-11-03T15:54:06.927158Z"
    },
    "_cell_guid": "c1d874fa-5ea5-edbb-726c-ae98c84e6120"
   },
   "outputs": [],
   "source": [
    "X = data.iloc[:, data.columns != 'Class']\n",
    "y = data.iloc[:, data.columns == 'Class']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  UNDER-sampling \n",
    "\n",
    "Apply UNDER-sampling by randomly selecting x samples from the majority class (0), where x is the total number of records with the minority class (1). \n",
    "\n",
    "The under-sampled dataset has a 50/50 class ratio of samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T15:54:27.473931Z",
     "start_time": "2018-11-03T15:54:27.402424Z"
    },
    "_cell_guid": "2af7c203-44ed-66b6-6141-ac0d0637fcc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total transactions in under_sample_data: 984\n",
      "Normal transactions: 492\n",
      "Fraud transactions: 492\n"
     ]
    }
   ],
   "source": [
    "# Picking the indices of the minority (fraud) class\n",
    "fraud_indices = np.array(data[data.Class == 1].index)\n",
    "\n",
    "# Picking the indices of the normal class\n",
    "normal_indices = np.array(data[data.Class == 0].index)\n",
    "\n",
    "# Number of data points in the minority (fraud) class\n",
    "N_records_fraud = len(fraud_indices)\n",
    "\n",
    "\n",
    "# Out of the normal class indices, randomly select N_records_fraud samples \n",
    "random_normal_indices = np.random.choice(normal_indices, N_records_fraud, replace = False)\n",
    "\n",
    "\n",
    "# Appending the indices of normal and fraud classes\n",
    "under_sample_indices = np.concatenate([fraud_indices, random_normal_indices])\n",
    "\n",
    "# Under sample dataset\n",
    "under_sample_data = data.iloc[under_sample_indices,:]\n",
    "\n",
    "# Check if the under-sampled Data is balanced \n",
    "num_normal = sum(under_sample_data[\"Class\"] == 0)\n",
    "num_fraud = sum(under_sample_data[\"Class\"] == 1)\n",
    "\n",
    "# Total number of transactions in the under_sample_data ? => ANSWER:  984\n",
    "print(\"Total transactions in under_sample_data:\", len(under_sample_data))\n",
    "\n",
    "# Number of normal under sample transactions ? =>   ANSWER:  492\n",
    "print(\"Normal transactions:\", num_normal)\n",
    "\n",
    "# Number of fraud transactions? => ANSWER:  492\n",
    "print(\"Fraud transactions:\", num_fraud)\n",
    "\n",
    "# Extract the features in matrix X_undersample, the class labels in vector  y_undersample\n",
    "X_undersample = under_sample_data.iloc[:, under_sample_data.columns != 'Class']\n",
    "y_undersample = under_sample_data.iloc[:, under_sample_data.columns == 'Class']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6b74ba73-82a8-3585-b790-44fe486ff19d"
   },
   "source": [
    "### Explanation of random_state\n",
    "\n",
    "All computers have what is called a pseudo-random number generator. This is something that produces seemingly random numbers, but if kept being repeated, would reproduce the same sequence eventually.\n",
    "Where the number generator is started is known as the seed. When you specify the random_state parameter, you are just setting the random seed for the random number generator.\n",
    "\n",
    "Suppose you set random_seed = 0. The random number generator might then produce the sequence of integers\n",
    "0, 19, 11, 2, 34, 5, 23, 24, 0, 1, 89, …\n",
    "\n",
    "and by fixing random_state=0, you will always see this sequence each time you call your train_test_split function. \n",
    "\n",
    "On the other hand, suppose you set random_state=1 and got the following sequence of integers:\n",
    "91, 18, 11, 34, 34, 5, 19, 18, 0, 0, 1, …\n",
    "\n",
    "You will always see these random numbers when you set random_state = 1. \n",
    "\n",
    "### Train-test data splitting\n",
    "\n",
    "Apply *train_test_split* to the Whole dataset and to the Undersampled dataset with 30% train-test data ratio and random_state = 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T15:55:48.985351Z",
     "start_time": "2018-11-03T15:55:48.794082Z"
    },
    "_cell_guid": "4a725b16-c14a-2be8-8240-617b7b2ed8cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHOLE DATA:\n",
      "Train dataset size: 199364\n",
      "Test dataset size: 85443\n",
      "\n",
      "UNDER-SAMPLED DATA:\n",
      "Train dataset size: 688\n",
      "Test dataset size: 296\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Call function train_test_split to devide the WHOLE dataset \n",
    "# in 30% for test data and the rest for training data. \n",
    "# add random_state=1 in order to start from the same random values and compare results\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "print(\"WHOLE DATA:\")\n",
    "\n",
    "# train dataset ?  => ANSWER: 199364\n",
    "print(\"Train dataset size:\", len(X_train))\n",
    "\n",
    "# test dataset ? => ANSWER: 85443\n",
    "print(\"Test dataset size:\", len(X_test))\n",
    "\n",
    "\n",
    "\n",
    "# DO the same division for Undersampled dataset\n",
    "X_train_undersample, X_test_undersample, y_train_undersample, y_test_undersample = train_test_split(X_undersample, y_undersample, test_size=0.3, random_state=1)\n",
    "\n",
    "print() \n",
    "print(\"UNDER-SAMPLED DATA:\")\n",
    "\n",
    "# train dataset ?  \n",
    "print(\"Train dataset size:\", len(X_train_undersample))  # Expected: 688\n",
    "\n",
    "# test dataset ? \n",
    "print(\"Test dataset size:\", len(X_test_undersample))  # Expected: 296\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6927cc69-57e6-4f34-4680-0b0016d414a0"
   },
   "source": [
    "###  MODEL 1: Logistic regression classifier - Undersampled data\n",
    "\n",
    "- Accuracy = (TP+TN)/total\n",
    "- Precision = TP/(TP+FP)\n",
    "- Recall = TP/(TP+FN)\n",
    "\n",
    "**Our goal is, do not miss a fraud transaction**, therefore  we are interested in the Recall score, because that is the metric to capture the most fraudulent transactions. Due to the imbalacing of the data, many observations could be predicted as False Negatives, that is, we predict a normal transaction, but it is in fact a fraudulent one. Recall captures this.\n",
    "\n",
    "Precision is less important metric for this problem, because if we predict that a transaction is fraudulent but it is not (this is false positive case), this is not a massive problem compared to the opposite. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T00:35:02.476042Z",
     "start_time": "2018-11-04T00:35:02.469768Z"
    },
    "_cell_guid": "9c7ec815-da54-993b-ef8d-b41b767cfacf"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix,precision_recall_curve,auc,roc_auc_score,roc_curve,recall_score,classification_report "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "88765ef8-cb56-860a-d249-9691d90afcb2"
   },
   "source": [
    "### K-fold Cross Validation (CV) to find the best hyper-parameter C of Logistic Regression.  \n",
    "\n",
    "C =1/$\\lambda$, where $\\lambda$ is the regularization parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T00:42:39.453151Z",
     "start_time": "2018-11-04T00:42:39.440375Z"
    },
    "_cell_guid": "069bc837-cfd1-006e-c589-7085d5d29a8e"
   },
   "outputs": [],
   "source": [
    "# Find the best hyper-parameter C. Optimizing for recall perf. metric \n",
    "def print_gridsearch_scores(x_train_data,y_train_data):\n",
    "    c_param_range = [0.01,0.1,1,10]\n",
    "\n",
    "    clf = GridSearchCV(LogisticRegression(), {\"C\": c_param_range}, cv=5, scoring='recall')\n",
    "    clf.fit(x_train_data,y_train_data)\n",
    "\n",
    "    print(\"Best hyper-parameter C\")\n",
    "    print(clf.best_params_)\n",
    "\n",
    "    print(\"K-fold Score (Recall):\")\n",
    "    \n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    \n",
    "    # K-fold Recall results for different values of C\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "        \n",
    "    return clf.best_params_[\"C\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T00:42:40.091983Z",
     "start_time": "2018-11-04T00:42:39.798627Z"
    },
    "_cell_guid": "983c1c75-8092-9a8e-40ca-754fde9e2301"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameter C\n",
      "{'C': 10}\n",
      "K-fold Score (Recall):\n",
      "0.871 (+/-0.060) for {'C': 0.01}\n",
      "0.906 (+/-0.050) for {'C': 0.1}\n",
      "0.920 (+/-0.029) for {'C': 1}\n",
      "0.923 (+/-0.043) for {'C': 10}\n"
     ]
    }
   ],
   "source": [
    "#Apply print_gridsearch_scores to get the best C with the Undersampled dataset\n",
    "best_c = print_gridsearch_scores(X_train_undersample,y_train_undersample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e40b554a-5b88-2828-f655-d54560ad7480"
   },
   "source": [
    "### Model 1.1: Logistic Regression trained and tested with undersampled data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T00:42:41.489547Z",
     "start_time": "2018-11-04T00:42:41.241496Z"
    },
    "_cell_guid": "5c8e4c0e-8cfd-7422-04a8-1b47b8531267"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix (undersample test dataset)\n",
      "[[149   5]\n",
      " [  9 133]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9366197183098591"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use best C to train LogReg model with undersampled train data and \n",
    "# test it with undersampled test data \n",
    "\n",
    "lr = LogisticRegression(C = best_c)\n",
    "lr.fit(X_train_undersample,y_train_undersample)\n",
    "y_pred_undersample = lr.predict(X_test_undersample)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test_undersample,y_pred_undersample)\n",
    "\n",
    "print('Confusion matrix (undersample test dataset)')\n",
    "print(cnf_matrix)\n",
    "\n",
    "#Compute Recall metric\n",
    "recall_score(y_test_undersample, y_pred_undersample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3aee694e-c838-434a-0cb6-dafaa3a0b224"
   },
   "source": [
    "\n",
    "### Model 1.2: Logistic Regression trained on under-sampled data and tested with the whole test data\n",
    "\n",
    "Apply the same approach as above. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T00:42:43.940100Z",
     "start_time": "2018-11-04T00:42:43.645024Z"
    },
    "_cell_guid": "2fac80a6-cc45-49e8-3fd6-2322e2461955"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameter C\n",
      "{'C': 10}\n",
      "K-fold Score (Recall):\n",
      "0.597 (+/-0.111) for {'C': 0.01}\n",
      "0.619 (+/-0.093) for {'C': 0.1}\n",
      "0.627 (+/-0.091) for {'C': 1}\n",
      "0.630 (+/-0.091) for {'C': 10}\n",
      "Confusion matrix (undersample test dataset)\n",
      "[[154   0]\n",
      " [ 48  94]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IPython -- An enhanced Interactive Python\n",
      "=========================================\n",
      "\n",
      "IPython offers a fully compatible replacement for the standard Python\n",
      "interpreter, with convenient shell features, special commands, command\n",
      "history mechanism and output results caching.\n",
      "\n",
      "At your system command line, type 'ipython -h' to see the command line\n",
      "options available. This document only describes interactive features.\n",
      "\n",
      "GETTING HELP\n",
      "------------\n",
      "\n",
      "Within IPython you have various way to access help:\n",
      "\n",
      "  ?         -> Introduction and overview of IPython's features (this screen).\n",
      "  object?   -> Details about 'object'.\n",
      "  object??  -> More detailed, verbose information about 'object'.\n",
      "  %quickref -> Quick reference of all IPython specific syntax and magics.\n",
      "  help      -> Access Python's own help system.\n",
      "\n",
      "If you are in terminal IPython you can quit this screen by pressing `q`.\n",
      "\n",
      "\n",
      "MAIN FEATURES\n",
      "-------------\n",
      "\n",
      "* Access to the standard Python help with object docstrings and the Python\n",
      "  manuals. Simply type 'help' (no quotes) to invoke it.\n",
      "\n",
      "* Magic commands: type %magic for information on the magic subsystem.\n",
      "\n",
      "* System command aliases, via the %alias command or the configuration file(s).\n",
      "\n",
      "* Dynamic object information:\n",
      "\n",
      "  Typing ?word or word? prints detailed information about an object. Certain\n",
      "  long strings (code, etc.) get snipped in the center for brevity.\n",
      "\n",
      "  Typing ??word or word?? gives access to the full information without\n",
      "  snipping long strings. Strings that are longer than the screen are printed\n",
      "  through the less pager.\n",
      "\n",
      "  The ?/?? system gives access to the full source code for any object (if\n",
      "  available), shows function prototypes and other useful information.\n",
      "\n",
      "  If you just want to see an object's docstring, type '%pdoc object' (without\n",
      "  quotes, and without % if you have automagic on).\n",
      "\n",
      "* Tab completion in the local namespace:\n",
      "\n",
      "  At any time, hitting tab will complete any available python commands or\n",
      "  variable names, and show you a list of the possible completions if there's\n",
      "  no unambiguous one. It will also complete filenames in the current directory.\n",
      "\n",
      "* Search previous command history in multiple ways:\n",
      "\n",
      "  - Start typing, and then use arrow keys up/down or (Ctrl-p/Ctrl-n) to search\n",
      "    through the history items that match what you've typed so far.\n",
      "\n",
      "  - Hit Ctrl-r: opens a search prompt. Begin typing and the system searches\n",
      "    your history for lines that match what you've typed so far, completing as\n",
      "    much as it can.\n",
      "\n",
      "  - %hist: search history by index.\n",
      "\n",
      "* Persistent command history across sessions.\n",
      "\n",
      "* Logging of input with the ability to save and restore a working session.\n",
      "\n",
      "* System shell with !. Typing !ls will run 'ls' in the current directory.\n",
      "\n",
      "* The reload command does a 'deep' reload of a module: changes made to the\n",
      "  module since you imported will actually be available without having to exit.\n",
      "\n",
      "* Verbose and colored exception traceback printouts. See the magic xmode and\n",
      "  xcolor functions for details (just type %magic).\n",
      "\n",
      "* Input caching system:\n",
      "\n",
      "  IPython offers numbered prompts (In/Out) with input and output caching. All\n",
      "  input is saved and can be retrieved as variables (besides the usual arrow\n",
      "  key recall).\n",
      "\n",
      "  The following GLOBAL variables always exist (so don't overwrite them!):\n",
      "  _i: stores previous input.\n",
      "  _ii: next previous.\n",
      "  _iii: next-next previous.\n",
      "  _ih : a list of all input _ih[n] is the input from line n.\n",
      "\n",
      "  Additionally, global variables named _i<n> are dynamically created (<n>\n",
      "  being the prompt counter), such that _i<n> == _ih[<n>]\n",
      "\n",
      "  For example, what you typed at prompt 14 is available as _i14 and _ih[14].\n",
      "\n",
      "  You can create macros which contain multiple input lines from this history,\n",
      "  for later re-execution, with the %macro function.\n",
      "\n",
      "  The history function %hist allows you to see any part of your input history\n",
      "  by printing a range of the _i variables. Note that inputs which contain\n",
      "  magic functions (%) appear in the history with a prepended comment. This is\n",
      "  because they aren't really valid Python code, so you can't exec them.\n",
      "\n",
      "* Output caching system:\n",
      "\n",
      "  For output that is returned from actions, a system similar to the input\n",
      "  cache exists but using _ instead of _i. Only actions that produce a result\n",
      "  (NOT assignments, for example) are cached. If you are familiar with\n",
      "  Mathematica, IPython's _ variables behave exactly like Mathematica's %\n",
      "  variables.\n",
      "\n",
      "  The following GLOBAL variables always exist (so don't overwrite them!):\n",
      "  _ (one underscore): previous output.\n",
      "  __ (two underscores): next previous.\n",
      "  ___ (three underscores): next-next previous.\n",
      "\n",
      "  Global variables named _<n> are dynamically created (<n> being the prompt\n",
      "  counter), such that the result of output <n> is always available as _<n>.\n",
      "\n",
      "  Finally, a global dictionary named _oh exists with entries for all lines\n",
      "  which generated output.\n",
      "\n",
      "* Directory history:\n",
      "\n",
      "  Your history of visited directories is kept in the global list _dh, and the\n",
      "  magic %cd command can be used to go to any entry in that list.\n",
      "\n",
      "* Auto-parentheses and auto-quotes (adapted from Nathan Gray's LazyPython)\n",
      "\n",
      "  1. Auto-parentheses\n",
      "        \n",
      "     Callable objects (i.e. functions, methods, etc) can be invoked like\n",
      "     this (notice the commas between the arguments)::\n",
      "       \n",
      "         In [1]: callable_ob arg1, arg2, arg3\n",
      "       \n",
      "     and the input will be translated to this::\n",
      "       \n",
      "         callable_ob(arg1, arg2, arg3)\n",
      "       \n",
      "     This feature is off by default (in rare cases it can produce\n",
      "     undesirable side-effects), but you can activate it at the command-line\n",
      "     by starting IPython with `--autocall 1`, set it permanently in your\n",
      "     configuration file, or turn on at runtime with `%autocall 1`.\n",
      "\n",
      "     You can force auto-parentheses by using '/' as the first character\n",
      "     of a line.  For example::\n",
      "       \n",
      "          In [1]: /globals             # becomes 'globals()'\n",
      "       \n",
      "     Note that the '/' MUST be the first character on the line!  This\n",
      "     won't work::\n",
      "       \n",
      "          In [2]: print /globals    # syntax error\n",
      "\n",
      "     In most cases the automatic algorithm should work, so you should\n",
      "     rarely need to explicitly invoke /. One notable exception is if you\n",
      "     are trying to call a function with a list of tuples as arguments (the\n",
      "     parenthesis will confuse IPython)::\n",
      "       \n",
      "          In [1]: zip (1,2,3),(4,5,6)  # won't work\n",
      "       \n",
      "     but this will work::\n",
      "       \n",
      "          In [2]: /zip (1,2,3),(4,5,6)\n",
      "          ------> zip ((1,2,3),(4,5,6))\n",
      "          Out[2]= [(1, 4), (2, 5), (3, 6)]\n",
      "\n",
      "     IPython tells you that it has altered your command line by\n",
      "     displaying the new command line preceded by -->.  e.g.::\n",
      "       \n",
      "          In [18]: callable list\n",
      "          -------> callable (list)\n",
      "\n",
      "  2. Auto-Quoting\n",
      "    \n",
      "     You can force auto-quoting of a function's arguments by using ',' as\n",
      "     the first character of a line.  For example::\n",
      "       \n",
      "          In [1]: ,my_function /home/me   # becomes my_function(\"/home/me\")\n",
      "\n",
      "     If you use ';' instead, the whole argument is quoted as a single\n",
      "     string (while ',' splits on whitespace)::\n",
      "       \n",
      "          In [2]: ,my_function a b c   # becomes my_function(\"a\",\"b\",\"c\")\n",
      "          In [3]: ;my_function a b c   # becomes my_function(\"a b c\")\n",
      "\n",
      "     Note that the ',' MUST be the first character on the line!  This\n",
      "     won't work::\n",
      "       \n",
      "          In [4]: x = ,my_function /home/me    # syntax error\n"
     ]
    }
   ],
   "source": [
    "# Apply the same approach to train LogReg model with undersampled train dataset \n",
    "# but test it with WHOLE test dataset\n",
    "# Use best C to train LogReg model with undersampled train data and \n",
    "# test it with undersampled test data \n",
    "\n",
    "best_c = print_gridsearch_scores(X_train, y_train)\n",
    "\n",
    "lr = LogisticRegression(C = best_c)\n",
    "lr.fit(X_train_undersample,y_train_undersample)\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test,y_pred)\n",
    "\n",
    "print('Confusion matrix (undersample test dataset)')\n",
    "print(cnf_matrix)\n",
    "\n",
    "# Compute and print Recall metric\n",
    "recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "29bf4990-1b01-52fc-6c37-d4414b0aafa7"
   },
   "source": [
    "###  ROC curve & AUC\n",
    "\n",
    "Plot the Receiver Operating Characteristic (ROC) curve and compute the Area Under the ROC Curve (AUC). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m lr \u001b[38;5;241m=\u001b[39m LogisticRegression(C \u001b[38;5;241m=\u001b[39m best_c, penalty \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mlr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_undersample\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train_undersample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m y_pred_undersample_score\u001b[38;5;241m=\u001b[39mlr\u001b[38;5;241m.\u001b[39mdecision_function(X_test_undersample)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Compute ROC curve\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1193\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;124;03m    Fit the model according to the given training data.\u001b[39;00m\n\u001b[1;32m   1167\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;124;03m    The SAGA solver supports both float64 and float32 bit arrays.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1193\u001b[0m     solver \u001b[38;5;241m=\u001b[39m \u001b[43m_check_solver\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdual\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpenalty \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melasticnet\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml1_ratio \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1196\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1197\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml1_ratio parameter is only used when penalty is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1198\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melasticnet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1199\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(penalty=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpenalty)\n\u001b[1;32m   1200\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:63\u001b[0m, in \u001b[0;36m_check_solver\u001b[0;34m(solver, penalty, dual)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_check_solver\u001b[39m(solver, penalty, dual):\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m solver \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaga\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m penalty \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 63\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     64\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSolver \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msolver\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m supports only \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml2\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or None penalties, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpenalty\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpenalty.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     66\u001b[0m         )\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m solver \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m dual:\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSolver \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msolver\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m supports only dual=False, got dual=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdual\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty."
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(C = best_c, penalty = 'l1')\n",
    "lr.fit(X_train_undersample,y_train_undersample)\n",
    "y_pred_undersample_score=lr.decision_function(X_test_undersample)\n",
    "\n",
    "# Compute ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test_undersample,y_pred_undersample_score)\n",
    "\n",
    "# Compute Area Under the ROC Curve (AUC), it is a scalar \n",
    "roc_auc = auc(fpr,tpr)\n",
    "\n",
    "# Plot ROC\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b',label='AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.0])\n",
    "plt.ylim([-0.1,1.01])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1f1e40a4-bca8-87ad-f368-d0b50e7d6158"
   },
   "source": [
    "#### REMARK\n",
    "To create the undersampled data, we randomly picked some samples from the majority class. This is a valid technique, however is doesn't represent the real (huge) population. \n",
    "For sufficient statistical credibility, it would be usefull to repeat the process with different undersampled configurations and check if the previous chosen parameters are still the most effective. In the end, the idea is to use a wider random representation of the whole dataset and rely on the averaged best parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2e0e6bfc-37ac-2d2e-61af-7118619fdf27"
   },
   "source": [
    "### MODEL 2: Logistic regression classifier - Skewed data\n",
    "\n",
    "Now, apply K-fold Cross Validation (CV) to find the best hyper-parameter C with whole train data, as it was done above. \n",
    "\n",
    "K-fold is now computationally much more time consuming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T00:43:31.501891Z",
     "start_time": "2018-11-04T00:42:50.753355Z"
    },
    "_cell_guid": "2aaf245f-43cd-d543-b857-562fb696fc4e"
   },
   "outputs": [],
   "source": [
    "best_c = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the best C to train LogReg model with the whole train data and test it with whole test data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T00:43:40.964169Z",
     "start_time": "2018-11-04T00:43:35.816138Z"
    },
    "_cell_guid": "634c1907-a5c5-888c-c2e9-da73f81ee445"
   },
   "outputs": [],
   "source": [
    "lr = ?\n",
    "?\n",
    "y_pred = ?\n",
    "\n",
    "# Compute and print confusion matrix\n",
    "?\n",
    "\n",
    "# Compute and print Recall metric.\n",
    "?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_change_revision": 4,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
